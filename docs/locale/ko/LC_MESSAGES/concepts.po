# SOME DESCRIPTIVE TITLE.
# Copyright (C)
# This file is distributed under the same license as the Airflow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Airflow 1.10.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-04-03 15:37+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../concepts.rst:19
msgid "Concepts"
msgstr "개념"

#: ../../concepts.rst:21
msgid ""
"The Airflow platform is a tool for describing, executing, and monitoring "
"workflows."
msgstr ""
"에어플로우 플랫폼은 워크플로우를 서술, 실행, 감시하는 도구이다."

#: ../../concepts.rst:25
msgid "Core Ideas"
msgstr "핵심 개념"

#: ../../concepts.rst:28
msgid "DAGs"
msgstr ""

#: ../../concepts.rst:30
msgid ""
"In Airflow, a ``DAG`` -- or a Directed Acyclic Graph -- is a collection "
"of all the tasks you want to run, organized in a way that reflects their "
"relationships and dependencies."
msgstr ""

#: ../../concepts.rst:34
msgid ""
"For example, a simple DAG could consist of three tasks: A, B, and C. It "
"could say that A has to run successfully before B can run, but C can run "
"anytime. It could say that task A times out after 5 minutes, and B can be"
" restarted up to 5 times in case it fails. It might also say that the "
"workflow will run every night at 10pm, but shouldn't start until a "
"certain date."
msgstr ""

#: ../../concepts.rst:40
msgid ""
"In this way, a DAG describes *how* you want to carry out your workflow; "
"but notice that we haven't said anything about *what* we actually want to"
" do! A, B, and C could be anything. Maybe A prepares data for B to "
"analyze while C sends an email. Or perhaps A monitors your location so B "
"can open your garage door while C turns on your house lights. The "
"important thing is that the DAG isn't concerned with what its constituent"
" tasks do; its job is to make sure that whatever they do happens at the "
"right time, or in the right order, or with the right handling of any "
"unexpected issues."
msgstr ""

#: ../../concepts.rst:49
msgid ""
"DAGs are defined in standard Python files that are placed in Airflow's "
"``DAG_FOLDER``. Airflow will execute the code in each file to dynamically"
" build the ``DAG`` objects. You can have as many DAGs as you want, each "
"describing an arbitrary number of tasks. In general, each one should "
"correspond to a single logical workflow."
msgstr ""

#: ../../concepts.rst:55
msgid ""
"When searching for DAGs, Airflow only considers python files that contain"
" the strings \"airflow\" and \"DAG\" by default. To consider all python "
"files instead, disable the ``DAG_DISCOVERY_SAFE_MODE`` configuration "
"flag."
msgstr ""

#: ../../concepts.rst:61
msgid "Scope"
msgstr ""

#: ../../concepts.rst:63
msgid ""
"Airflow will load any ``DAG`` object it can import from a DAGfile. "
"Critically, that means the DAG must appear in ``globals()``. Consider the"
" following two DAGs. Only ``dag_1`` will be loaded; the other one only "
"appears in a local scope."
msgstr ""

#: ../../concepts.rst:77
msgid ""
"Sometimes this can be put to good use. For example, a common pattern with"
" ``SubDagOperator`` is to define the subdag inside a function so that "
"Airflow doesn't try to load it as a standalone DAG."
msgstr ""

#: ../../concepts.rst:82
msgid "Default Arguments"
msgstr ""

#: ../../concepts.rst:84
msgid ""
"If a dictionary of ``default_args`` is passed to a DAG, it will apply "
"them to any of its operators. This makes it easy to apply a common "
"parameter to many operators without having to type it many times."
msgstr ""

#: ../../concepts.rst:99
msgid "Context Manager"
msgstr ""

#: ../../concepts.rst:101 ../../concepts.rst:159 ../../concepts.rst:186
msgid "*Added in Airflow 1.8*"
msgstr ""

#: ../../concepts.rst:103
msgid ""
"DAGs can be used as context managers to automatically assign new "
"operators to that DAG."
msgstr ""

#: ../../concepts.rst:115
msgid "Operators"
msgstr ""

#: ../../concepts.rst:117
msgid ""
"While DAGs describe *how* to run a workflow, ``Operators`` determine what"
" actually gets done."
msgstr ""

#: ../../concepts.rst:120
msgid ""
"An operator describes a single task in a workflow. Operators are usually "
"(but not always) atomic, meaning they can stand on their own and don't "
"need to share resources with any other operators. The DAG will make sure "
"that operators run in the correct certain order; other than those "
"dependencies, operators generally run independently. In fact, they may "
"run on two completely different machines."
msgstr ""

#: ../../concepts.rst:126
msgid ""
"This is a subtle but very important point: in general, if two operators "
"need to share information, like a filename or small amount of data, you "
"should consider combining them into a single operator. If it absolutely "
"can't be avoided, Airflow does have a feature for operator cross-"
"communication called XCom that is described elsewhere in this document."
msgstr ""

#: ../../concepts.rst:132
msgid "Airflow provides operators for many common tasks, including:"
msgstr ""

#: ../../concepts.rst:134
msgid ""
":class:`~airflow.operators.bash_operator.BashOperator` - executes a bash "
"command"
msgstr ""

#: ../../concepts.rst:135
msgid ""
":class:`~airflow.operators.python_operator.PythonOperator` - calls an "
"arbitrary Python function"
msgstr ""

#: ../../concepts.rst:136
msgid ":class:`~airflow.operators.email_operator.EmailOperator` - sends an email"
msgstr ""

#: ../../concepts.rst:137
msgid ""
":class:`~airflow.operators.http_operator.SimpleHttpOperator` - sends an "
"HTTP request"
msgstr ""

#: ../../concepts.rst:138
msgid ""
":class:`~airflow.operators.mysql_operator.MySqlOperator`, "
":class:`~airflow.operators.sqlite_operator.SqliteOperator`, "
":class:`~airflow.operators.postgres_operator.PostgresOperator`, "
":class:`~airflow.operators.mssql_operator.MsSqlOperator`, "
":class:`~airflow.operators.oracle_operator.OracleOperator`, "
":class:`~airflow.operators.jdbc_operator.JdbcOperator`, etc. - executes a"
" SQL command"
msgstr ""

#: ../../concepts.rst:144
msgid "``Sensor`` - waits for a certain time, file, database row, S3 key, etc..."
msgstr ""

#: ../../concepts.rst:146
msgid ""
"In addition to these basic building blocks, there are many more specific "
"operators: :class:`~airflow.operators.docker_operator.DockerOperator`, "
":class:`~airflow.operators.hive_operator.HiveOperator`, "
":class:`~airflow.operators.s3_file_transform_operator.S3FileTransformOperator(`,"
" :class:`~airflow.operators.presto_to_mysql.PrestoToMySqlTransfer`, "
":class:`~airflow.operators.slack_operator.SlackAPIOperator`... you get "
"the idea!"
msgstr ""

#: ../../concepts.rst:152
msgid "Operators are only loaded by Airflow if they are assigned to a DAG."
msgstr ""

#: ../../concepts.rst:154
msgid "See :doc:`howto/operator/index` for how to use Airflow operators."
msgstr ""

#: ../../concepts.rst:157
msgid "DAG Assignment"
msgstr ""

#: ../../concepts.rst:161
msgid ""
"Operators do not have to be assigned to DAGs immediately (previously "
"``dag`` was a required argument). However, once an operator is assigned "
"to a DAG, it can not be transferred or unassigned. DAG assignment can be "
"done explicitly when the operator is created, through deferred "
"assignment, or even inferred from other operators."
msgstr ""

#: ../../concepts.rst:184
msgid "Bitshift Composition"
msgstr ""

#: ../../concepts.rst:188
msgid ""
"Traditionally, operator relationships are set with the ``set_upstream()``"
" and ``set_downstream()`` methods. In Airflow 1.8, this can be done with "
"the Python bitshift operators ``>>`` and ``<<``. The following four "
"statements are all functionally equivalent:"
msgstr ""

#: ../../concepts.rst:201
msgid ""
"When using the bitshift to compose operators, the relationship is set in "
"the direction that the bitshift operator points. For example, ``op1 >> "
"op2`` means that ``op1`` runs first and ``op2`` runs second. Multiple "
"operators can be composed -- keep in mind the chain is executed left-to-"
"right and the rightmost object is always returned. For example:"
msgstr ""

#: ../../concepts.rst:211 ../../concepts.rst:225 ../../concepts.rst:253
msgid "is equivalent to:"
msgstr ""

#: ../../concepts.rst:219
msgid ""
"For convenience, the bitshift operators can also be used with DAGs. For "
"example:"
msgstr ""

#: ../../concepts.rst:232
msgid "We can put this all together to build a simple pipeline:"
msgstr ""

#: ../../concepts.rst:247
msgid "Bitshift can also be used with lists. For example:"
msgstr ""

#: ../../concepts.rst:260
msgid "and equivalent to:"
msgstr ""

#: ../../concepts.rst:267
msgid "Tasks"
msgstr ""

#: ../../concepts.rst:269
msgid ""
"Once an operator is instantiated, it is referred to as a \"task\". The "
"instantiation defines specific values when calling the abstract operator,"
" and the parameterized task becomes a node in a DAG."
msgstr ""

#: ../../concepts.rst:274
msgid "Task Instances"
msgstr ""

#: ../../concepts.rst:276
msgid ""
"A task instance represents a specific run of a task and is characterized "
"as the combination of a DAG, a task, and a point in time. Task instances "
"also have an indicative state, which could be \"running\", \"success\", "
"\"failed\", \"skipped\", \"up for retry\", etc."
msgstr ""

#: ../../concepts.rst:282
msgid "Task Lifecycle"
msgstr ""

#: ../../concepts.rst:284
msgid ""
"A task goes through various stages from start to completion. In the "
"Airflow UI (graph and tree views), these stages are displayed by a color "
"representing each stage:"
msgstr ""

#: ../../concepts.rst:290
msgid "The happy flow consists of the following stages:"
msgstr ""

#: ../../concepts.rst:292
msgid "no status (scheduler created empty task instance)"
msgstr ""

#: ../../concepts.rst:293
msgid "queued (scheduler placed a task to run on the queue)"
msgstr ""

#: ../../concepts.rst:294
msgid "running (worker picked up a task and is now running it)"
msgstr ""

#: ../../concepts.rst:295
msgid "success (task completed)"
msgstr ""

#: ../../concepts.rst:297
msgid ""
"There is also visual difference between scheduled and manually triggered "
"DAGs/tasks:"
msgstr ""

#: ../../concepts.rst:302
msgid ""
"The DAGs/tasks with a black border are scheduled runs, whereas the non-"
"bordered DAGs/tasks are manually triggered, i.e. by `airflow "
"trigger_dag`."
msgstr ""

#: ../../concepts.rst:306
msgid "Workflows"
msgstr ""

#: ../../concepts.rst:308
msgid ""
"You're now familiar with the core building blocks of Airflow. Some of the"
" concepts may sound very similar, but the vocabulary can be "
"conceptualized like this:"
msgstr ""

#: ../../concepts.rst:312
msgid "DAG: a description of the order in which work should take place"
msgstr ""

#: ../../concepts.rst:313
msgid "Operator: a class that acts as a template for carrying out some work"
msgstr ""

#: ../../concepts.rst:314
msgid "Task: a parameterized instance of an operator"
msgstr ""

#: ../../concepts.rst:315
msgid ""
"Task Instance: a task that 1) has been assigned to a DAG and 2) has a "
"state associated with a specific run of the DAG"
msgstr ""

#: ../../concepts.rst:318
msgid ""
"By combining ``DAGs`` and ``Operators`` to create ``TaskInstances``, you "
"can build complex workflows."
msgstr ""

#: ../../concepts.rst:322
msgid "Additional Functionality"
msgstr ""

#: ../../concepts.rst:324
msgid ""
"In addition to the core Airflow objects, there are a number of more "
"complex features that enable behaviors like limiting simultaneous access "
"to resources, cross-communication, conditional execution, and more."
msgstr ""

#: ../../concepts.rst:329
msgid "Hooks"
msgstr ""

#: ../../concepts.rst:331
msgid ""
"Hooks are interfaces to external platforms and databases like Hive, S3, "
"MySQL, Postgres, HDFS, and Pig. Hooks implement a common interface when "
"possible, and act as a building block for operators. They also use the "
"``airflow.models.connection.Connection`` model to retrieve hostnames and "
"authentication information. Hooks keep authentication code and "
"information out of pipelines, centralized in the metadata database."
msgstr ""

#: ../../concepts.rst:338
msgid ""
"Hooks are also very useful on their own to use in Python scripts, Airflow"
" airflow.operators.PythonOperator, and in interactive environments like "
"iPython or Jupyter Notebook."
msgstr ""

#: ../../concepts.rst:343
msgid "Pools"
msgstr ""

#: ../../concepts.rst:345
msgid ""
"Some systems can get overwhelmed when too many processes hit them at the "
"same time. Airflow pools can be used to **limit the execution "
"parallelism** on arbitrary sets of tasks. The list of pools is managed in"
" the UI (``Menu -> Admin -> Pools``) by giving the pools a name and "
"assigning it a number of worker slots. Tasks can then be associated with "
"one of the existing pools by using the ``pool`` parameter when creating "
"tasks (i.e., instantiating operators)."
msgstr ""

#: ../../concepts.rst:363
msgid ""
"The ``pool`` parameter can be used in conjunction with "
"``priority_weight`` to define priorities in the queue, and which tasks "
"get executed first as slots open up in the pool. The default "
"``priority_weight`` is ``1``, and can be bumped to any number. When "
"sorting the queue to evaluate which task should be executed next, we use "
"the ``priority_weight``, summed up with all of the ``priority_weight`` "
"values from tasks downstream from this task. You can use this to bump a "
"specific important task and the whole path to that task gets prioritized "
"accordingly."
msgstr ""

#: ../../concepts.rst:373
msgid ""
"Tasks will be scheduled as usual while the slots fill up. Once capacity "
"is reached, runnable tasks get queued and their state will show as such "
"in the UI. As slots free up, queued tasks start running based on the "
"``priority_weight`` (of the task and its descendants)."
msgstr ""

#: ../../concepts.rst:378
msgid ""
"Note that by default tasks aren't assigned to any pool and their "
"execution parallelism is only limited to the executor's setting."
msgstr ""

#: ../../concepts.rst:381
msgid "To combine Pools with SubDAGs see the `SubDAGs`_ section."
msgstr ""

#: ../../concepts.rst:386
msgid "Connections"
msgstr ""

#: ../../concepts.rst:388
msgid ""
"The connection information to external systems is stored in the Airflow "
"metadata database and managed in the UI (``Menu -> Admin -> "
"Connections``). A ``conn_id`` is defined there and hostname / login / "
"password / schema information attached to it. Airflow pipelines can "
"simply refer to the centrally managed ``conn_id`` without having to hard "
"code any of this information anywhere."
msgstr ""

#: ../../concepts.rst:395
msgid ""
"Many connections with the same ``conn_id`` can be defined and when that "
"is the case, and when the **hooks** uses the ``get_connection`` method "
"from ``BaseHook``, Airflow will choose one connection randomly, allowing "
"for some basic load balancing and fault tolerance when used in "
"conjunction with retries."
msgstr ""

#: ../../concepts.rst:401
msgid ""
"Airflow also has the ability to reference connections via environment "
"variables from the operating system. Then connection parameters must be "
"saved in URI format."
msgstr ""

#: ../../concepts.rst:405
msgid ""
"If connections with the same ``conn_id`` are defined in both Airflow "
"metadata database and environment variables, only the one in environment "
"variables will be referenced by Airflow (for example, given ``conn_id`` "
"``postgres_master``, Airflow will search for "
"``AIRFLOW_CONN_POSTGRES_MASTER`` in environment variables first and "
"directly reference it if found, before it starts to search in metadata "
"database)."
msgstr ""

#: ../../concepts.rst:412
msgid ""
"Many hooks have a default ``conn_id``, where operators using that hook do"
" not need to supply an explicit connection ID. For example, the default "
"``conn_id`` for the :class:`~airflow.hooks.postgres_hook.PostgresHook` is"
" ``postgres_default``."
msgstr ""

#: ../../concepts.rst:417
msgid ""
"See :doc:`howto/connection/index` for how to create and manage "
"connections."
msgstr ""

#: ../../concepts.rst:420
msgid "Queues"
msgstr ""

#: ../../concepts.rst:422
msgid ""
"When using the CeleryExecutor, the Celery queues that tasks are sent to "
"can be specified. ``queue`` is an attribute of BaseOperator, so any task "
"can be assigned to any queue. The default queue for the environment is "
"defined in the ``airflow.cfg``'s ``celery -> default_queue``. This "
"defines the queue that tasks get assigned to when not specified, as well "
"as which queue Airflow workers listen to when started."
msgstr ""

#: ../../concepts.rst:429
msgid ""
"Workers can listen to one or multiple queues of tasks. When a worker is "
"started (using the command ``airflow worker``), a set of comma-delimited "
"queue names can be specified (e.g. ``airflow worker -q spark``). This "
"worker will then only pick up tasks wired to the specified queue(s)."
msgstr ""

#: ../../concepts.rst:434
msgid ""
"This can be useful if you need specialized workers, either from a "
"resource perspective (for say very lightweight tasks where one worker "
"could take thousands of tasks without a problem), or from an environment "
"perspective (you want a worker running from within the Spark cluster "
"itself because it needs a very specific environment and security rights)."
msgstr ""

#: ../../concepts.rst:441
msgid "XComs"
msgstr ""

#: ../../concepts.rst:443
msgid ""
"XComs let tasks exchange messages, allowing more nuanced forms of control"
" and shared state. The name is an abbreviation of \"cross-"
"communication\". XComs are principally defined by a key, value, and "
"timestamp, but also track attributes like the task/DAG that created the "
"XCom and when it should become visible. Any object that can be pickled "
"can be used as an XCom value, so users should make sure to use objects of"
" appropriate size."
msgstr ""

#: ../../concepts.rst:450
msgid ""
"XComs can be \"pushed\" (sent) or \"pulled\" (received). When a task "
"pushes an XCom, it makes it generally available to other tasks. Tasks can"
" push XComs at any time by calling the ``xcom_push()`` method. In "
"addition, if a task returns a value (either from its Operator's "
"``execute()`` method, or from a PythonOperator's ``python_callable`` "
"function), then an XCom containing that value is automatically pushed."
msgstr ""

#: ../../concepts.rst:457
msgid ""
"Tasks call ``xcom_pull()`` to retrieve XComs, optionally applying filters"
" based on criteria like ``key``, source ``task_ids``, and source "
"``dag_id``. By default, ``xcom_pull()`` filters for the keys that are "
"automatically given to XComs when they are pushed by being returned from "
"execute functions (as opposed to XComs that are pushed manually)."
msgstr ""

#: ../../concepts.rst:463
msgid ""
"If ``xcom_pull`` is passed a single string for ``task_ids``, then the "
"most recent XCom value from that task is returned; if a list of "
"``task_ids`` is passed, then a corresponding list of XCom values is "
"returned."
msgstr ""

#: ../../concepts.rst:477
msgid ""
"It is also possible to pull XCom directly in a template, here's an "
"example of what this may look like:"
msgstr ""

#: ../../concepts.rst:484
msgid ""
"Note that XComs are similar to `Variables`_, but are specifically "
"designed for inter-task communication rather than global settings."
msgstr ""

#: ../../concepts.rst:489
msgid "Variables"
msgstr ""

#: ../../concepts.rst:491
msgid ""
"Variables are a generic way to store and retrieve arbitrary content or "
"settings as a simple key value store within Airflow. Variables can be "
"listed, created, updated and deleted from the UI (``Admin -> "
"Variables``), code or CLI. In addition, json settings files can be bulk "
"uploaded through the UI. While your pipeline code definition and most of "
"your constants and variables should be defined in code and stored in "
"source control, it can be useful to have some variables or configuration "
"items accessible and modifiable through the UI."
msgstr ""

#: ../../concepts.rst:508
msgid ""
"The second call assumes ``json`` content and will be deserialized into "
"``bar``. Note that ``Variable`` is a sqlalchemy model and can be used as "
"such. The third call uses the ``default_var`` parameter with the value "
"``None``, which either returns an existing value or ``None`` if the "
"variable isn't defined. The get function will throw a ``KeyError`` if the"
" variable doesn't exist and no default is provided."
msgstr ""

#: ../../concepts.rst:515
msgid "You can use a variable from a jinja template with the syntax :"
msgstr ""

#: ../../concepts.rst:521
msgid "or if you need to deserialize a json object from the variable :"
msgstr ""

#: ../../concepts.rst:529
msgid "Branching"
msgstr ""

#: ../../concepts.rst:531
msgid ""
"Sometimes you need a workflow to branch, or only go down a certain path "
"based on an arbitrary condition which is typically related to something "
"that happened in an upstream task. One way to do this is by using the "
"``BranchPythonOperator``."
msgstr ""

#: ../../concepts.rst:536
msgid ""
"The ``BranchPythonOperator`` is much like the PythonOperator except that "
"it expects a ``python_callable`` that returns a task_id (or list of "
"task_ids). The task_id returned is followed, and all of the other paths "
"are skipped. The task_id returned by the Python function has to reference"
" a task directly downstream from the BranchPythonOperator task."
msgstr ""

#: ../../concepts.rst:542
msgid ""
"Note that when a path is a downstream task of the returned task (list), "
"it will not be skipped:"
msgstr ""

#: ../../concepts.rst:547
msgid ""
"Paths of the branching task are ``branch_a``, ``join`` and ``branch_b``. "
"Since ``join`` is a downstream task of ``branch_a``, it will be excluded "
"from the skipped tasks when ``branch_a`` is returned by the Python "
"callable."
msgstr ""

#: ../../concepts.rst:551
msgid ""
"The ``BranchPythonOperator`` can also be used with XComs allowing "
"branching context to dynamically decide what branch to follow based on "
"previous tasks. For example:"
msgstr ""

#: ../../concepts.rst:583
msgid "SubDAGs"
msgstr ""

#: ../../concepts.rst:585
msgid ""
"SubDAGs are perfect for repeating patterns. Defining a function that "
"returns a DAG object is a nice design pattern when using Airflow."
msgstr ""

#: ../../concepts.rst:588
msgid ""
"Airbnb uses the *stage-check-exchange* pattern when loading data. Data is"
" staged in a temporary table, after which data quality checks are "
"performed against that table. Once the checks all pass the partition is "
"moved into the production table."
msgstr ""

#: ../../concepts.rst:593
msgid "As another example, consider the following DAG:"
msgstr ""

#: ../../concepts.rst:597
msgid ""
"We can combine all of the parallel ``task-*`` operators into a single "
"SubDAG, so that the resulting DAG resembles the following:"
msgstr ""

#: ../../concepts.rst:602
msgid ""
"Note that SubDAG operators should contain a factory method that returns a"
" DAG object. This will prevent the SubDAG from being treated like a "
"separate DAG in the main UI. For example:"
msgstr ""

#: ../../concepts.rst:628
msgid "This SubDAG can then be referenced in your main DAG file:"
msgstr ""

#: ../../concepts.rst:655
msgid ""
"You can zoom into a SubDagOperator from the graph view of the main DAG to"
" show the tasks contained within the SubDAG:"
msgstr ""

#: ../../concepts.rst:660
msgid "Some other tips when using SubDAGs:"
msgstr ""

#: ../../concepts.rst:662
msgid ""
"by convention, a SubDAG's ``dag_id`` should be prefixed by its parent and"
" a dot. As in ``parent.child``"
msgstr ""

#: ../../concepts.rst:664
msgid ""
"share arguments between the main DAG and the SubDAG by passing arguments "
"to the SubDAG operator (as demonstrated above)"
msgstr ""

#: ../../concepts.rst:666
msgid ""
"SubDAGs must have a schedule and be enabled. If the SubDAG's schedule is "
"set to ``None`` or ``@once``, the SubDAG will succeed without having done"
" anything"
msgstr ""

#: ../../concepts.rst:669
msgid "clearing a SubDagOperator also clears the state of the tasks within"
msgstr ""

#: ../../concepts.rst:670
msgid ""
"marking success on a SubDagOperator does not affect the state of the "
"tasks within"
msgstr ""

#: ../../concepts.rst:672
msgid ""
"refrain from using ``depends_on_past=True`` in tasks within the SubDAG as"
" this can be confusing"
msgstr ""

#: ../../concepts.rst:674
msgid ""
"it is possible to specify an executor for the SubDAG. It is common to use"
" the SequentialExecutor if you want to run the SubDAG in-process and "
"effectively limit its parallelism to one. Using LocalExecutor can be "
"problematic as it may over-subscribe your worker, running multiple tasks "
"in a single slot"
msgstr ""

#: ../../concepts.rst:680
msgid "See ``airflow/example_dags`` for a demonstration."
msgstr ""

#: ../../concepts.rst:682
msgid ""
"Note that airflow pool is not honored by SubDagOperator. Hence resources "
"could be consumed by SubdagOperators."
msgstr ""

#: ../../concepts.rst:686
msgid "SLAs"
msgstr ""

#: ../../concepts.rst:688
msgid ""
"Service Level Agreements, or time by which a task or DAG should have "
"succeeded, can be set at a task level as a ``timedelta``. If one or many "
"instances have not succeeded by that time, an alert email is sent "
"detailing the list of tasks that missed their SLA. The event is also "
"recorded in the database and made available in the web UI under "
"``Browse->SLA Misses`` where events can be analyzed and documented."
msgstr ""

#: ../../concepts.rst:696
msgid "Email Configuration"
msgstr ""

#: ../../concepts.rst:698
msgid ""
"You can configure the email that is being sent in your ``airflow.cfg`` by"
" setting a ``subject_template`` and/or a ``html_content_template`` in the"
" ``email`` section."
msgstr ""

#: ../../concepts.rst:711
msgid ""
"To access the task's information you use `Jinja Templating "
"<http://jinja.pocoo.org/docs/dev/>`_  in your template files."
msgstr ""

#: ../../concepts.rst:713
msgid "For example a ``html_content_template`` file could look like this:"
msgstr ""

#: ../../concepts.rst:725
msgid "Trigger Rules"
msgstr ""

#: ../../concepts.rst:727
msgid ""
"Though the normal workflow behavior is to trigger tasks when all their "
"directly upstream tasks have succeeded, Airflow allows for more complex "
"dependency settings."
msgstr ""

#: ../../concepts.rst:731
msgid ""
"All operators have a ``trigger_rule`` argument which defines the rule by "
"which the generated task get triggered. The default value for "
"``trigger_rule`` is ``all_success`` and can be defined as \"trigger this "
"task when all directly upstream tasks have succeeded\". All other rules "
"described here are based on direct parent tasks and are values that can "
"be passed to any operator while creating tasks:"
msgstr ""

#: ../../concepts.rst:738
msgid "``all_success``: (default) all parents have succeeded"
msgstr ""

#: ../../concepts.rst:739
msgid ""
"``all_failed``: all parents are in a ``failed`` or ``upstream_failed`` "
"state"
msgstr ""

#: ../../concepts.rst:740
msgid "``all_done``: all parents are done with their execution"
msgstr ""

#: ../../concepts.rst:741
msgid ""
"``one_failed``: fires as soon as at least one parent has failed, it does "
"not wait for all parents to be done"
msgstr ""

#: ../../concepts.rst:742
msgid ""
"``one_success``: fires as soon as at least one parent succeeds, it does "
"not wait for all parents to be done"
msgstr ""

#: ../../concepts.rst:743
msgid ""
"``none_failed``: all parents have not failed (``failed`` or "
"``upstream_failed``) i.e. all parents have succeeded or been skipped"
msgstr ""

#: ../../concepts.rst:744
msgid "``dummy``: dependencies are just for show, trigger at will"
msgstr ""

#: ../../concepts.rst:746
msgid ""
"Note that these can be used in conjunction with ``depends_on_past`` "
"(boolean) that, when set to ``True``, keeps a task from getting triggered"
" if the previous schedule for the task hasn't succeeded."
msgstr ""

#: ../../concepts.rst:750
msgid ""
"One must be aware of the interaction between trigger rules and skipped "
"tasks in schedule level. Skipped tasks will cascade through trigger rules"
" ``all_success`` and ``all_failed`` but not ``all_done``, ``one_failed``,"
" ``one_success``, ``none_failed`` and ``dummy``."
msgstr ""

#: ../../concepts.rst:755 ../../concepts.rst:827
msgid "For example, consider the following DAG:"
msgstr ""

#: ../../concepts.rst:789
msgid ""
"In the case of this DAG, ``join`` is downstream of ``follow_branch_a`` "
"and ``branch_false``. The ``join`` task will show up as skipped because "
"its ``trigger_rule`` is set to ``all_success`` by default and skipped "
"tasks will cascade through ``all_success``."
msgstr ""

#: ../../concepts.rst:796
msgid "By setting ``trigger_rule`` to ``none_failed`` in ``join`` task,"
msgstr ""

#: ../../concepts.rst:805
msgid ""
"The ``join`` task will be triggered as soon as ``branch_false`` has been "
"skipped (a valid completion state) and ``follow_branch_a`` has succeeded."
" Because skipped tasks **will not** cascade through ``none_failed``."
msgstr ""

#: ../../concepts.rst:813
msgid "Latest Run Only"
msgstr ""

#: ../../concepts.rst:815
msgid ""
"Standard workflow behavior involves running a series of tasks for a "
"particular date/time range. Some workflows, however, perform tasks that "
"are independent of run time but need to be run on a schedule, much like a"
" standard cron job. In these cases, backfills or running jobs missed "
"during a pause just wastes CPU cycles."
msgstr ""

#: ../../concepts.rst:821
msgid ""
"For situations like this, you can use the ``LatestOnlyOperator`` to skip "
"tasks that are not being run during the most recent scheduled run for a "
"DAG. The ``LatestOnlyOperator`` skips all downstream tasks, if the time "
"right now is not between its ``execution_time`` and the next scheduled "
"``execution_time``."
msgstr ""

#: ../../concepts.rst:860
msgid ""
"In the case of this DAG, the ``latest_only`` task will show up as skipped"
" for all runs except the latest run. ``task1`` is directly downstream of "
"``latest_only`` and will also skip for all runs except the latest. "
"``task2`` is entirely independent of ``latest_only`` and will run in all "
"scheduled periods. ``task3`` is downstream of ``task1`` and ``task2`` and"
" because of the default ``trigger_rule`` being ``all_success`` will "
"receive a cascaded skip from ``task1``. ``task4`` is downstream of "
"``task1`` and ``task2``. It will be first skipped directly by "
"``LatestOnlyOperator``, even its ``trigger_rule`` is set to ``all_done``."
msgstr ""

#: ../../concepts.rst:874
msgid "Zombies & Undeads"
msgstr ""

#: ../../concepts.rst:876
msgid ""
"Task instances die all the time, usually as part of their normal life "
"cycle, but sometimes unexpectedly."
msgstr ""

#: ../../concepts.rst:879
msgid ""
"Zombie tasks are characterized by the absence of an heartbeat (emitted by"
" the job periodically) and a ``running`` status in the database. They can"
" occur when a worker node can't reach the database, when Airflow "
"processes are killed externally, or when a node gets rebooted for "
"instance. Zombie killing is performed periodically by the scheduler's "
"process."
msgstr ""

#: ../../concepts.rst:886
msgid ""
"Undead processes are characterized by the existence of a process and a "
"matching heartbeat, but Airflow isn't aware of this task as ``running`` "
"in the database. This mismatch typically occurs as the state of the "
"database is altered, most likely by deleting rows in the \"Task "
"Instances\" view in the UI. Tasks are instructed to verify their state as"
" part of the heartbeat routine, and terminate themselves upon figuring "
"out that they are in this \"undead\" state."
msgstr ""

#: ../../concepts.rst:896
msgid "Cluster Policy"
msgstr ""

#: ../../concepts.rst:898
msgid ""
"Your local Airflow settings file can define a ``policy`` function that "
"has the ability to mutate task attributes based on other task or DAG "
"attributes. It receives a single argument as a reference to task objects,"
" and is expected to alter its attributes."
msgstr ""

#: ../../concepts.rst:903
msgid ""
"For example, this function could apply a specific queue property when "
"using a specific operator, or enforce a task timeout policy, making sure "
"that no tasks run for more than 48 hours. Here's an example of what this "
"may look like inside your ``airflow_settings.py``:"
msgstr ""

#: ../../concepts.rst:919
msgid "Documentation & Notes"
msgstr ""

#: ../../concepts.rst:921
msgid ""
"It's possible to add documentation or notes to your DAGs & task objects "
"that become visible in the web interface (\"Graph View\" for DAGs, \"Task"
" Details\" for tasks). There are a set of special task attributes that "
"get rendered as rich content if defined:"
msgstr ""

#: ../../concepts.rst:927
msgid "attribute"
msgstr ""

#: ../../concepts.rst:927
msgid "rendered to"
msgstr ""

#: ../../concepts.rst:929
msgid "doc"
msgstr ""

#: ../../concepts.rst:929
msgid "monospace"
msgstr ""

#: ../../concepts.rst:930
msgid "doc_json"
msgstr ""

#: ../../concepts.rst:930
msgid "json"
msgstr ""

#: ../../concepts.rst:931
msgid "doc_yaml"
msgstr ""

#: ../../concepts.rst:931
msgid "yaml"
msgstr ""

#: ../../concepts.rst:932
msgid "doc_md"
msgstr ""

#: ../../concepts.rst:932
msgid "markdown"
msgstr ""

#: ../../concepts.rst:933
msgid "doc_rst"
msgstr ""

#: ../../concepts.rst:933
msgid "reStructuredText"
msgstr ""

#: ../../concepts.rst:936
msgid "Please note that for DAGs, doc_md is the only attribute interpreted."
msgstr ""

#: ../../concepts.rst:938
msgid ""
"This is especially useful if your tasks are built dynamically from "
"configuration files, it allows you to expose the configuration that led "
"to the related tasks in Airflow."
msgstr ""

#: ../../concepts.rst:957
msgid ""
"This content will get rendered as markdown respectively in the \"Graph "
"View\" and \"Task Details\" pages."
msgstr ""

#: ../../concepts.rst:963
msgid "Jinja Templating"
msgstr ""

#: ../../concepts.rst:965
msgid ""
"Airflow leverages the power of `Jinja Templating "
"<http://jinja.pocoo.org/docs/dev/>`_ and this can be a powerful tool to "
"use in combination with macros (see the :doc:`macros` section)."
msgstr ""

#: ../../concepts.rst:969
msgid ""
"For example, say you want to pass the execution date as an environment "
"variable to a Bash script using the ``BashOperator``."
msgstr ""

#: ../../concepts.rst:982
msgid ""
"Here, ``{{ ds }}`` is a macro, and because the ``env`` parameter of the "
"``BashOperator`` is templated with Jinja, the execution date will be "
"available as an environment variable named ``EXECUTION_DATE`` in your "
"Bash script."
msgstr ""

#: ../../concepts.rst:986
msgid ""
"You can use Jinja templating with every parameter that is marked as "
"\"templated\" in the documentation. Template substitution occurs just "
"before the pre_execute function of your operator is called."
msgstr ""

#: ../../concepts.rst:991
msgid "Packaged DAGs"
msgstr ""

#: ../../concepts.rst:992
msgid ""
"While often you will specify DAGs in a single ``.py`` file it might "
"sometimes be required to combine a DAG and its dependencies. For example,"
" you might want to combine several DAGs together to version them together"
" or you might want to manage them together or you might need an extra "
"module that is not available by default on the system you are running "
"Airflow on. To allow this you can create a zip file that contains the "
"DAG(s) in the root of the zip file and have the extra modules unpacked in"
" directories."
msgstr ""

#: ../../concepts.rst:1000
msgid "For instance you can create a zip file that looks like this:"
msgstr ""

#: ../../concepts.rst:1009
msgid ""
"Airflow will scan the zip file and try to load ``my_dag1.py`` and "
"``my_dag2.py``. It will not go into subdirectories as these are "
"considered to be potential packages."
msgstr ""

#: ../../concepts.rst:1013
msgid ""
"In case you would like to add module dependencies to your DAG you "
"basically would do the same, but then it is more to use a virtualenv and "
"pip."
msgstr ""

#: ../../concepts.rst:1029
msgid ""
"the zip file will be inserted at the beginning of module search list "
"(sys.path) and as such it will be available to any other code that "
"resides within the same interpreter."
msgstr ""

#: ../../concepts.rst:1033
msgid "packaged dags cannot be used with pickling turned on."
msgstr ""

#: ../../concepts.rst:1035
msgid ""
"packaged dags cannot contain dynamic libraries (eg. libz.so) these need "
"to be available on the system if a module needs those. In other words "
"only pure python modules can be packaged."
msgstr ""

#: ../../concepts.rst:1041
msgid ".airflowignore"
msgstr ""

#: ../../concepts.rst:1043
msgid ""
"A ``.airflowignore`` file specifies the directories or files in "
"``DAG_FOLDER`` that Airflow should intentionally ignore. Each line in "
"``.airflowignore`` specifies a regular expression pattern, and "
"directories or files whose names (not DAG id) match any of the patterns "
"would be ignored (under the hood, ``re.findall()`` is used to match the "
"pattern). Overall it works like a ``.gitignore`` file."
msgstr ""

#: ../../concepts.rst:1050
msgid ""
"``.airflowignore`` file should be put in your ``DAG_FOLDER``. For "
"example, you can prepare a ``.airflowignore`` file with contents"
msgstr ""

#: ../../concepts.rst:1059
msgid ""
"Then files like \"project_a_dag_1.py\", \"TESTING_project_a.py\", "
"\"tenant_1.py\", \"project_a/dag_1.py\", and \"tenant_1/dag_1.py\" in "
"your ``DAG_FOLDER`` would be ignored (If a directory's name matches any "
"of the patterns, this directory and all its subfolders would not be "
"scanned by Airflow at all. This improves efficiency of DAG finding)."
msgstr ""

#: ../../concepts.rst:1064
msgid ""
"The scope of a ``.airflowignore`` file is the directory it is in plus all"
" its subfolders. You can also prepare ``.airflowignore`` file for a "
"subfolder in ``DAG_FOLDER`` and it would only be applicable for that "
"subfolder."
msgstr ""

